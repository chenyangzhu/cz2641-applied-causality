{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SC(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, N, P, T):\n",
    "    \"\"\"\n",
    "    h: p x 1\n",
    "    L: N x T\n",
    "    delta: T x 1\n",
    "    gamma: N x 1\n",
    "    \"\"\"\n",
    "    super(SC, self).__init__()\n",
    "    \n",
    "    self.h = torch.zeros(P, 1, requires_grad=True)\n",
    "    self.L = torch.zeros(N, T, requires_grad=True)\n",
    "    self.delta = torch.zeros(T, 1, requires_grad=True)\n",
    "    self.gamma = torch.zeros(N, 1, requires_grad=True)\n",
    "    \n",
    "    self.N = N\n",
    "    self.P = P\n",
    "    self.T = T\n",
    "    \n",
    "    self.lambda_L = 1\n",
    "    self.lambda_h = 1\n",
    "        \n",
    "  def forward(self, X, Y):\n",
    "    \"\"\"\n",
    "    X: N x P\n",
    "    \"\"\"\n",
    "    \n",
    "    Xh = torch.matmul(X, self.h)  # N x 1\n",
    "    gamma1 = torch.matmul(self.gamma, torch.ones(1, self.T)) # N x T\n",
    "    delta1 = torch.matmul(torch.ones(self.N, 1), self.delta.T)\n",
    "    \n",
    "    loss = torch.norm(Y - self.L - Xh - gamma1 - delta1, 2) / (self.N * self.T)\n",
    "    \n",
    "    loss_penalty = loss + self.lambda_L * torch.norm(self.L, 'nuc') + self.lambda_h * torch.sum(torch.abs(self.h))    \n",
    "\n",
    "    return loss_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "P = 10\n",
    "T = 100\n",
    "\n",
    "X = torch.ones(N, P)\n",
    "Y = torch.ones(N, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SC(N, P, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([model.h, model.L, model.delta, model.gamma],\n",
    "                            lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "  optimizer.zero_grad()\n",
    "  loss = model.forward(X+1, Y)\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1445e-03,  3.2464e-05,  3.2493e-05,  ...,  3.2471e-05,\n",
       "          3.2464e-05,  3.2598e-05],\n",
       "        [ 3.2501e-05, -5.1444e-03,  3.2492e-05,  ...,  3.2579e-05,\n",
       "          3.2556e-05,  3.2560e-05],\n",
       "        [ 3.2497e-05,  3.2555e-05, -5.1442e-03,  ...,  3.2544e-05,\n",
       "          3.2513e-05,  3.2610e-05],\n",
       "        ...,\n",
       "        [-1.5553e-06, -1.5721e-06, -1.5653e-06,  ..., -1.5649e-06,\n",
       "         -1.5641e-06, -1.5676e-06],\n",
       "        [-1.5553e-06, -1.5721e-06, -1.5653e-06,  ..., -1.5649e-06,\n",
       "         -1.5641e-06, -1.5676e-06],\n",
       "        [-1.5553e-06, -1.5721e-06, -1.5653e-06,  ..., -1.5649e-06,\n",
       "         -1.5641e-06, -1.5676e-06]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
